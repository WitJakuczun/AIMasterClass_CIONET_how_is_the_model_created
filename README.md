# Fine-tuning Transformer (and not only) models for sentiment analysis

This project provides a CLI application to fine-tune a series of NLP models for sentiment analysis. It uses a cross-validation strategy to train and evaluate the models.


# ML Model lifecycle 
Application handles two stages of ML Model life cycle:
- Model development
- Model usage

## Model development
- creating code many ML Models in separate folders
- create an ML experiment for a given training dataset
- evalute ML Models' perfomance using selected experiment
- compare ML Models's performance for given experiment

## Model 
- train selected ML Model on the whole train dataset
- predict using trained ML Model on a new predict dataset

# Main technologies and technology stack

- **Python 3.11+**: The main programming language.
- **Typer**: For building the CLI interface.
- **Loguru**: For logging.
- **Pandas**: For data manipulation.
- **Scikit-learn**: For basic ML tasks, including data splitting and evaluation.
- **Transformers**: A library from Hugging Face for working with Transformer models (e.g., RoBERTa, Gemma).
- **Pydantic**: For data validation and settings management.
- **PEFT (Parameter-Efficient Fine-Tuning)**: For efficient model tuning.
- **TRL (Transformer Reinforcement Learning)**: For training models using reinforcement learning.
- **Pytest**: For writing and running tests.
- **uv**: For managing dependencies and the virtual environment.

# Application components

## ML Experiment
ML experiment:
- has unique id
- connects to CV Splits of training dataset
- contains many evaluation runs of different ML models
- allows to run all evaluation runs into a pandas dataframe 
- uses `pydantic` for configuration validation.

## CV Splits
Given training dataset a CV Split is created by splitting the dataset into list of train/val datasets.
The default splitting is Stratified KFold split with K as a parameter.
The splitting code can be customized.

## Evaluation step
Given predictions and test dataset the evaluation step calculates perfomance measures.

## Evaluation Run

Each evaluation run is defined by:
- ML Model as described below (train and predict steps)
- Evaluation step
- Experiment with CV Splits

The evaluation run does the following:
- For each pair of train/val datasets from CV Splits:
    - Training
        - run Train step on train dataset
        - store trained model instnace in Experiment with model id and train/val split id
    - Predicting
        - run Predict step using val dataset and model instance from train step
        - store predictions in Experiment with model id and train/val split id
    - Evaluate
        - Run Evaluation Run on each train/val datasets and store metrics with model id and train/val split id in Evaluation Run
        - Run Evaluation Run on combined trian/val datasets and store metrics with model id in Evaluation Run


## ML Model Building Blocks

Code for each model consists of the two building blocks: train and predict.
The project can contain many models with their code stored in separate folders.

### Train step
Takes dataset, list of hyperparameters and runs a train function to generate model instance. The model instance is written in a given working directory.

### Predict step
Takes dataset, model instance generated by Train step and runs predict function. The generated ouptut is written in a given working directory.

# How to use the CLI

The application is now using a CLI based on `typer`. The main entry point is `main.py`.

**Note:** All commands should be run with `uv run` prefix. For example: `uv run python main.py list-models`.

## ML Model Lifecycle

The CLI is designed to support the entire ML model lifecycle, from experimentation to production.

### Stage 1: Experimentation

This stage is about finding the best model for your data. It involves creating dataset splits, running experiments, and comparing model performance.

#### Generate dataset splits

You can either create cross-validation splits or a single train/validation split.

To generate CV splits, you can use the `generate-cv-splits` command:

```bash
uv run python main.py generate-cv-splits --experiment-id <experiment_id> --n-splits <n_splits> --input-file <input_file> --target-column <target_column>
```

To generate a single train/validation split, you can use the `generate-split` command:

```bash
uv run python main.py generate-split --experiment-id <experiment_id> --input-file <input_file> --target-column <target_column> --train-ratio <train_ratio> --val-ratio <val_ratio>
```

#### Run a full experiment

To run a full experiment (train, predict, and evaluate for all folds), you can use the `run-experiment` command:

```bash
uv run python main.py run-experiment --experiment-id <experiment_id> --model-config-name <model_config_name> --model-output-dir <model_output_dir> --prediction-output-dir <prediction_output_dir>
```

You can also run the steps of an experiment individually:

*   **Train a model:**
    ```bash
    uv run python main.py train --experiment-id <experiment_id> --fold-number <fold_number> --model-config-name <model_config_name> --model-output-dir <model_output_dir>
    ```
*   **Predict:**
    ```bash
    uv run python main.py predict --experiment-id <experiment_id> --fold-number <fold_number> --model-config-name <model_config_name> --model-input-dir <model_input_dir> --prediction-output-dir <prediction_output_dir>
    ```
*   **Evaluate:**
    ```bash
    uv run python main.py evaluate --experiment-id <experiment_id> --fold-number <fold_number> --prediction-input-dir <prediction_input_dir>
    ```

#### Compare models

To compare the performance of different models for a given experiment, you can use the `compare-models` command:

```bash
uv run python main.py compare-models --experiment-id <experiment_id> --output-file <output_file>
```

### Stage 2: Production

Once you have found the best model, you can train it on the entire dataset and use it to make predictions on new data.

#### Train a final model

To train a final model on the entire training dataset, you can use the `train-final` command:

```bash
uv run python main.py train-final --experiment-id <experiment_id> --model-config-name <model_config_name> --model-output-dir <model_output_dir>
```

#### Predict on new data

To make predictions using a trained model on new data, you can use the `predict-new` command:

```bash
uv run python main.py predict-new --model-path <model_path> --input-file <input_file> --output-file <output_file>
```

## Extra Functionality

### List all available models

To see the list of all available models you can run the following command:
```bash
uv run python main.py list-models
```