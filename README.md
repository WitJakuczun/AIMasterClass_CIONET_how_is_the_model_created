# Fine-tuning Transformer (and not only) models for sentiment analysis

This project provides a CLI application to fine-tune a series of NLP models for sentiment analysis. It uses a cross-validation strategy to train and evaluate the models.


# ML Model lifecycle 
Application handles two stages of ML Model life cycle:
- Model development
- Model usage

## Model development
- creating code many ML Models in separate folders
- create an ML experiment for a given training dataset
- evalute ML Models' perfomance using selected experiment
- compare ML Models's performance for given experiment

## Model 
- train selected ML Model on the whole train dataset
- predict using trained ML Model on a new predict dataset

# Main technologies and technology stack

- **Python 3.11+**: The main programming language.
- **Typer**: For building the CLI interface.
- **Loguru**: For logging.
- **Pandas**: For data manipulation.
- **Scikit-learn**: For basic ML tasks, including data splitting and evaluation.
- **Transformers**: A library from Hugging Face for working with Transformer models (e.g., RoBERTa, Gemma).
- **PEFT (Parameter-Efficient Fine-Tuning)**: For efficient model tuning.
- **TRL (Transformer Reinforcement Learning)**: For training models using reinforcement learning.
- **Pytest**: For writing and running tests.
- **uv**: For managing dependencies and the virtual environment.

# Application components

## ML Experiment
ML experiment:
- has unique id
- connects to CV Splits of training dataset
- contains many evaluation runs of different ML models
- allows to run all evaluation runs into a pandas dataframe 
- uses `pydantic` for configuration validation.

## CV Splits
Given training dataset a CV Split is created by splitting the dataset into list of train/val datasets.
The default splitting is Stratified KFold split with K as a parameter.
The splitting code can be customized.

## Evaluation step
Given predictions and test dataset the evaluation step calculates perfomance measures.

## Evaluation Run

Each evaluation run is defined by:
- ML Model as described below (train and predict steps)
- Evaluation step
- Experiment with CV Splits

The evaluation run does the following:
- For each pair of train/val datasets from CV Splits:
    - Training
        - run Train step on train dataset
        - store trained model instnace in Experiment with model id and train/val split id
    - Predicting
        - run Predict step using val dataset and model instance from train step
        - store predictions in Experiment with model id and train/val split id
    - Evaluate
        - Run Evaluation Run on each train/val datasets and store metrics with model id and train/val split id in Evaluation Run
        - Run Evaluation Run on combined trian/val datasets and store metrics with model id in Evaluation Run


## ML Model Building Blocks

Code for each model consists of the two building blocks: train and predict.
The project can contain many models with their code stored in separate folders.

### Train step
Takes dataset, list of hyperparameters and runs a train function to generate model instance. The model instance is written in a given working directory.

### Predict step
Takes dataset, model instance generated by Train step and runs predict function. The generated ouptut is written in a given working directory.

# How to use the CLI

The application is now using a CLI based on `typer`. The main entry point is `main.py`.

**Note:** All commands should be run with `uv run` prefix. For example: `uv run python main.py list-models`.

## ML Model Lifecycle

The CLI is designed to support the entire ML model lifecycle, from experimentation to production.

### Stage 1: Experimentation

This stage is about finding the best model for your data. It involves creating dataset splits, running experiments, and comparing model performance.

#### 1. Generate dataset splits

First, generate your dataset splits using the `generate-splits` command. This command also defines your backtesting strategy.

```bash
uv run python main.py generate-splits --experiment-id <experiment_id> --input-file <input_file> --target-column <target_column> --test-size <test_size> --backtesting-strategy <backtesting_strategy> [--cv-folds <cv_folds>] [--backtesting-val-size <backtesting_val_size>] [--perf-estimation-val-size <perf_estimation_val_size>] [--final-model-val-size <final_model_val_size>]
```

*   `--experiment-id`: Unique ID for the experiment.
*   `--input-file`: Path to the input CSV file.
*   `--target-column`: Column to stratify on (e.g., 'Sentiment').
*   `--test-size`: Proportion of the dataset for the initial hold-out test set.
*   `--backtesting-strategy`: Choose your backtesting approach: `cv` for Cross-Validation or `train-val` for a single Train-Validation split.
*   `--cv-folds`: (Optional, required for `cv` strategy) Number of folds for cross-validation. Defaults to 5.
*   `--backtesting-val-size`: (Optional, required for `train-val` strategy) Validation set size for the backtesting train/val split. Defaults to 0.15.
*   `--perf-estimation-val-size`: (Optional) Validation set size for the performance estimation split. Defaults to 0.1.
*   `--final-model-val-size`: (Optional) Validation set size for the final model training split. Defaults to 0.1.

#### 2. Run Backtesting

Once splits are generated, run the backtesting process for a specific model.

```bash
uv run python main.py run-backtesting --experiment-id <experiment_id> --model-config-name <model_config_name>
```

#### 3. Estimate Performance

After backtesting, you can estimate the performance of the model on a hold-out set.

```bash
uv run python main.py estimate-performance --experiment-id <experiment_id> --model-config-name <model_config_name>
```

#### 4. Compare models

To compare the performance of different models for a given experiment, use the `compare-models` command. This command now supports comparing results from both backtesting and performance estimation runs.

```bash
uv run python main.py compare-models --experiment-id <experiment_id> [--output-file <output_file>] [--run-type <run_type>]
```

*   `--experiment-id`: Unique ID for the experiment.
*   `--output-file`: (Optional) Path to save the comparison report (CSV format).
*   `--run-type`: (Optional) Specifies which type of run to compare. Accepted values are `backtesting` or `performance_estimation`. If omitted, the command will compare both.

### Stage 2: Production

Once you have found the best model, you can train it on the entire dataset and use it to make predictions on new data.

#### Train a final model

To train a final model on the entire training dataset, you can use the `train-final-model` command:

```bash
uv run python main.py train-final-model --experiment-id <experiment_id> --model-config-name <model_config_name> --model-output-dir <model_output_dir>
```

#### Predict on new data

To make predictions using a trained model on new data, you can use the `predict-new` command:

```bash
uv run python main.py predict-new --model-path <model_path> --input-file <input_file> --output-file <output_file>
```

## Advanced: Manual Execution

You can also run the steps of an experiment individually for more granular control.

*   **Train a model:**
    ```bash
uv run python main.py train --experiment-id <experiment_id> --fold-number <fold_number> --model-config-name <model_config_name> --model-output-dir <model_output_dir>
    ```
*   **Predict:**
    ```bash
uv run python main.py predict --experiment-id <experiment_id> --fold-number <fold_number> --model-config-name <model_config_name> --model-input-dir <model_input_dir> --prediction-output-dir <prediction_output_dir>
    ```
*   **Evaluate:**
    ```bash
uv run python main.py evaluate --experiment-id <experiment_id> --fold-number <fold_number> --prediction-input-dir <prediction_input_dir>
    ```
*   **Run a full experiment (legacy):**
    ```bash
uv run python main.py run-experiment --experiment-id <experiment_id> --model-config-name <model_config_name> --model-output-dir <model_output_dir> --prediction-output-dir <prediction_output_dir>
    ```

## Extra Functionality

### List all available models

To see the list of all available models you can run the following command:
```bash
uv run python main.py list-models
```