{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pełny Cykl Życia Modelu Sentymentu\n",
    "\n",
    "Ten notebook krok po kroku demonstruje cały cykl życia modelu do klasyfikacji sentymentu, od przygotowania danych, przez eksperymenty i wybór najlepszego modelu, aż po jego użycie do predykcji na nowych danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sekcja 1: Przygotowanie i Konfiguracja\n",
    "\n",
    "Zakładamy, że dane treningowe zostały już przygotowane i znajdują się w pliku `artifacts/data/all_train.csv`. W tej komórce definiujemy zmienne, których będziemy używać w całym procesie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "os.environ['EXPERIMENT_ID'] = \"exp_notebook_tutorial\"\n",
    "os.environ['INPUT_DATA'] = \"artifacts/data/all_train.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sekcja 2: Generowanie Podziałów Danych\n",
    "\n",
    "Tworzymy podziały danych dla naszego eksperymentu. Użyjemy strategii `train-val` z wydzielonym zbiorem testowym (20% danych), który posłuży do ostatecznej oceny najlepszego modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run cli generate-splits --experiment-id $EXPERIMENT_ID --input-file $INPUT_DATA --test-size 0.2 --backtesting-strategy train-val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sekcja 3: Uruchomienie Eksperymentów i Porównanie Modeli\n",
    "\n",
    "Teraz uruchomimy proces treningu i walidacji dla każdego z wybranych modeli. Po każdym przebiegu wyświetlimy zaktualizowaną tabelę z porównaniem wyników."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: svm-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run cli run-backtesting --experiment-id $EXPERIMENT_ID --model-config-name svm-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-20 14:07:25.268\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate\u001b[0m:\u001b[36mcompare_models\u001b[0m:\u001b[36m79\u001b[0m - \u001b[1m--- Model Comparison for Experiment 'exp_notebook_tutorial' ---\u001b[0m\n",
      "\u001b[32m2025-10-20 14:07:25.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate\u001b[0m:\u001b[36mcompare_models\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1m\n",
      "--- Run Type: backtesting ---\u001b[0m\n",
      "\u001b[32m2025-10-20 14:07:25.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate\u001b[0m:\u001b[36mcompare_models\u001b[0m:\u001b[36m91\u001b[0m - \u001b[1mSummary for backtesting (train-val strategy):\u001b[0m\n",
      "\u001b[32m2025-10-20 14:07:25.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate\u001b[0m:\u001b[36mcompare_models\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1m\n",
      "             model_name  accuracy precision    recall  f1_score\n",
      "                             mean      mean      mean      mean\n",
      "0  prompt-gemma-3-1b-it  0.061966  0.712286  0.061966  0.113160\n",
      "1              svm-base  0.675214  0.669999  0.675214  0.671415\n",
      "2               svm-opt  0.705128  0.687651  0.705128  0.688811\u001b[0m\n",
      "\u001b[32m2025-10-20 14:07:25.276\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate\u001b[0m:\u001b[36mcompare_models\u001b[0m:\u001b[36m100\u001b[0m - \u001b[1mComparison report saved to artifacts/experiments/exp_notebook_tutorial/comparison.csv\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_type</th>\n",
       "      <th>model_name</th>\n",
       "      <th>fold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>backtesting</td>\n",
       "      <td>svm-opt</td>\n",
       "      <td>0</td>\n",
       "      <td>0.705128</td>\n",
       "      <td>0.687651</td>\n",
       "      <td>0.705128</td>\n",
       "      <td>0.688811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>backtesting</td>\n",
       "      <td>svm-base</td>\n",
       "      <td>0</td>\n",
       "      <td>0.675214</td>\n",
       "      <td>0.669999</td>\n",
       "      <td>0.675214</td>\n",
       "      <td>0.671415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>backtesting</td>\n",
       "      <td>prompt-gemma-3-1b-it</td>\n",
       "      <td>0</td>\n",
       "      <td>0.061966</td>\n",
       "      <td>0.712286</td>\n",
       "      <td>0.061966</td>\n",
       "      <td>0.113160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      run_type            model_name  fold  accuracy  precision    recall  \\\n",
       "0  backtesting               svm-opt     0  0.705128   0.687651  0.705128   \n",
       "1  backtesting              svm-base     0  0.675214   0.669999  0.675214   \n",
       "2  backtesting  prompt-gemma-3-1b-it     0  0.061966   0.712286  0.061966   \n",
       "\n",
       "   f1_score  \n",
       "0  0.688811  \n",
       "1  0.671415  \n",
       "2  0.113160  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!uv run cli compare-models --experiment-id $EXPERIMENT_ID --output-file artifacts/experiments/$EXPERIMENT_ID/comparison.csv --run-type backtesting\n",
    "comparison_df = pd.read_csv(f\"artifacts/experiments/{os.environ['EXPERIMENT_ID']}/comparison.csv\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: svm-opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run cli run-backtesting --experiment-id $EXPERIMENT_ID --model-config-name svm-opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run cli compare-models --experiment-id $EXPERIMENT_ID --output-file artifacts/experiments/$EXPERIMENT_ID/comparison.csv --run-type backtesting\n",
    "comparison_df = pd.read_csv(f\"artifacts/experiments/{os.environ['EXPERIMENT_ID']}/comparison.csv\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: Gemma Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-20 12:00:26.498\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmlops.runner\u001b[0m:\u001b[36mrun_backtesting\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1m--- Backtesting Fold 0 ---\u001b[0m\n",
      "\u001b[32m2025-10-20 12:00:26.498\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmlops.runner\u001b[0m:\u001b[36m_load_model_from_config\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mLoading model: PromptGemmaModel from models.prompt_gemma.model\u001b[0m\n",
      "\u001b[32m2025-10-20 12:00:26.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.prompt_gemma.model\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mUsing device: cpu\u001b[0m\n",
      "\u001b[32m2025-10-20 12:00:26.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmlops.runner\u001b[0m:\u001b[36mrun_backtesting\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mTraining model...\u001b[0m\n",
      "\u001b[32m2025-10-20 12:00:26.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.prompt_gemma.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mPromptGemmaModel does not require training. Skipping.\u001b[0m\n",
      "\u001b[32m2025-10-20 12:00:26.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.prompt_gemma.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m46\u001b[0m - \u001b[1mOutput directory ensured at: /Users/witjakuczunpriv/projects/AI_MasterClass_CIONET/artifacts/trained_models/exp_notebook_tutorial_backtesting/fold_0/prompt-gemma-3-1b-it\u001b[0m\n",
      "\u001b[32m2025-10-20 12:00:26.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmlops.runner\u001b[0m:\u001b[36m_save_model_spec\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mModel spec saved to /Users/witjakuczunpriv/projects/AI_MasterClass_CIONET/artifacts/trained_models/exp_notebook_tutorial_backtesting/fold_0/prompt-gemma-3-1b-it/model_spec.json\u001b[0m\n",
      "\u001b[32m2025-10-20 12:00:26.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmlops.runner\u001b[0m:\u001b[36mrun_backtesting\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1mPredicting...\u001b[0m\n",
      "\u001b[32m2025-10-20 12:00:26.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmodels.prompt_gemma.model\u001b[0m:\u001b[36m_load_model\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLoading base model for prompt-based prediction: google/gemma-1.1-2b-it\u001b[0m\n",
      "tokenizer_config.json: 100%|███████████████| 34.2k/34.2k [00:00<00:00, 51.5MB/s]\n",
      "tokenizer.model: 100%|█████████████████████| 4.24M/4.24M [00:01<00:00, 4.14MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 17.5M/17.5M [00:00<00:00, 49.2MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 636/636 [00:00<00:00, 5.63MB/s]\n",
      "config.json: 100%|█████████████████████████████| 618/618 [00:00<00:00, 4.52MB/s]\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "model.safetensors.index.json: 100%|████████| 13.5k/13.5k [00:00<00:00, 27.7MB/s]\n",
      "Fetching 2 files:   0%|                                   | 0/2 [00:00<?, ?it/s]\n",
      "model-00001-of-00002.safetensors:   0%|             | 0.00/4.95G [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   0%|             | 0.00/67.1M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  13%|▌   | 8.40M/67.1M [00:01<00:13, 4.29MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|    | 838k/4.95G [00:02<4:36:20, 298kB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|   | 2.30M/4.95G [00:04<2:14:13, 614kB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|   | 2.46M/4.95G [00:04<2:38:20, 520kB/s]\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:  13%|▌   | 8.40M/67.1M [00:20<00:13, 4.29MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|   | 2.46M/4.95G [00:20<2:38:20, 520kB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|   | 10.8M/4.95G [00:27<3:33:49, 385kB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|   | 23.4M/4.95G [00:36<1:51:21, 737kB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|  | 47.7M/4.95G [00:50<1:09:21, 1.18MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|  | 50.0M/4.95G [00:55<1:19:26, 1.03MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|     | 117M/4.95G [00:57<20:02, 4.01MB/s]\u001b[A^C\n",
      "Cancellation requested; stopping current tasks.\n",
      "model-00001-of-00002.safetensors:   2%|     | 117M/4.95G [01:06<45:25, 1.77MB/s]\n",
      "model-00002-of-00002.safetensors:  13%|▋    | 8.40M/67.1M [01:06<07:41, 127kB/s]\n"
     ]
    }
   ],
   "source": [
    "!uv run cli run-backtesting --experiment-id $EXPERIMENT_ID --model-config-name prompt-gemma-zeroshot-3-1b-it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: Gemma Few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!uv run cli run-backtesting --experiment-id $EXPERIMENT_ID --model-config-name prompt-gemma-fewshot-3-1b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!uv run cli compare-models --experiment-id $EXPERIMENT_ID --output-file artifacts/experiments/$EXPERIMENT_ID/comparison.csv --run-type backtesting\n",
    "comparison_df = pd.read_csv(f\"artifacts/experiments/{os.environ['EXPERIMENT_ID']}/comparison.csv\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: Gemma Few-shot v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!uv run cli run-backtesting --experiment-id $EXPERIMENT_ID --model-config-name prompt-gemma-fewshot-v2-3-1b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!uv run cli compare-models --experiment-id $EXPERIMENT_ID --output-file artifacts/experiments/$EXPERIMENT_ID/comparison.csv --run-type backtesting\n",
    "comparison_df = pd.read_csv(f\"artifacts/experiments/{os.environ['EXPERIMENT_ID']}/comparison.csv\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: bert-micro-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run cli run-backtesting --experiment-id $EXPERIMENT_ID --model-config-name bert-micro-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-20 14:57:28.765\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate\u001b[0m:\u001b[36mcompare_models\u001b[0m:\u001b[36m79\u001b[0m - \u001b[1m--- Model Comparison for Experiment 'exp_notebook_tutorial' ---\u001b[0m\n",
      "\u001b[32m2025-10-20 14:57:28.765\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate\u001b[0m:\u001b[36mcompare_models\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1m\n",
      "--- Run Type: backtesting ---\u001b[0m\n",
      "\u001b[32m2025-10-20 14:57:28.765\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate\u001b[0m:\u001b[36mcompare_models\u001b[0m:\u001b[36m91\u001b[0m - \u001b[1mSummary for backtesting (train-val strategy):\u001b[0m\n",
      "\u001b[32m2025-10-20 14:57:28.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate\u001b[0m:\u001b[36mcompare_models\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1m\n",
      "                      model_name  accuracy precision    recall  f1_score\n",
      "                                      mean      mean      mean      mean\n",
      "0           prompt-gemma-3-1b-it  0.606838  0.727688  0.606838  0.626360\n",
      "1   prompt-gemma-fewshot-3-1b-it  0.606838  0.727688  0.606838  0.626360\n",
      "2  prompt-gemma-zeroshot-3-1b-it  0.000000  0.000000  0.000000  0.000000\n",
      "3                       svm-base  0.675214  0.669999  0.675214  0.671415\n",
      "4                        svm-opt  0.705128  0.687651  0.705128  0.688811\u001b[0m\n",
      "\u001b[32m2025-10-20 14:57:28.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mevaluate\u001b[0m:\u001b[36mcompare_models\u001b[0m:\u001b[36m100\u001b[0m - \u001b[1mComparison report saved to artifacts/experiments/exp_notebook_tutorial/comparison.csv\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_type</th>\n",
       "      <th>model_name</th>\n",
       "      <th>fold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>backtesting</td>\n",
       "      <td>svm-opt</td>\n",
       "      <td>0</td>\n",
       "      <td>0.705128</td>\n",
       "      <td>0.687651</td>\n",
       "      <td>0.705128</td>\n",
       "      <td>0.688811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>backtesting</td>\n",
       "      <td>svm-base</td>\n",
       "      <td>0</td>\n",
       "      <td>0.675214</td>\n",
       "      <td>0.669999</td>\n",
       "      <td>0.675214</td>\n",
       "      <td>0.671415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>backtesting</td>\n",
       "      <td>prompt-gemma-3-1b-it</td>\n",
       "      <td>0</td>\n",
       "      <td>0.606838</td>\n",
       "      <td>0.727688</td>\n",
       "      <td>0.606838</td>\n",
       "      <td>0.626360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>backtesting</td>\n",
       "      <td>prompt-gemma-fewshot-3-1b-it</td>\n",
       "      <td>0</td>\n",
       "      <td>0.606838</td>\n",
       "      <td>0.727688</td>\n",
       "      <td>0.606838</td>\n",
       "      <td>0.626360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>backtesting</td>\n",
       "      <td>prompt-gemma-zeroshot-3-1b-it</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      run_type                     model_name  fold  accuracy  precision  \\\n",
       "0  backtesting                        svm-opt     0  0.705128   0.687651   \n",
       "1  backtesting                       svm-base     0  0.675214   0.669999   \n",
       "2  backtesting           prompt-gemma-3-1b-it     0  0.606838   0.727688   \n",
       "3  backtesting   prompt-gemma-fewshot-3-1b-it     0  0.606838   0.727688   \n",
       "4  backtesting  prompt-gemma-zeroshot-3-1b-it     0  0.000000   0.000000   \n",
       "\n",
       "     recall  f1_score  \n",
       "0  0.705128  0.688811  \n",
       "1  0.675214  0.671415  \n",
       "2  0.606838  0.626360  \n",
       "3  0.606838  0.626360  \n",
       "4  0.000000  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!uv run cli compare-models --experiment-id $EXPERIMENT_ID --output-file artifacts/experiments/$EXPERIMENT_ID/comparison.csv --run-type backtesting\n",
    "comparison_df = pd.read_csv(f\"artifacts/experiments/{os.environ['EXPERIMENT_ID']}/comparison.csv\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: bert-micro-long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run cli run-backtesting --experiment-id $EXPERIMENT_ID --model-config-name bert-micro-long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run cli compare-models --experiment-id $EXPERIMENT_ID --output-file artifacts/experiments/$EXPERIMENT_ID/comparison.csv --run-type backtesting\n",
    "comparison_df = pd.read_csv(f\"artifacts/experiments/{os.environ['EXPERIMENT_ID']}/comparison.csv\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run cli run-backtesting --experiment-id $EXPERIMENT_ID --model-config-name roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run cli compare-models --experiment-id $EXPERIMENT_ID --output-file artifacts/experiments/$EXPERIMENT_ID/comparison.csv --run-type backtesting\n",
    "comparison_df = pd.read_csv(f\"artifacts/experiments/{os.environ['EXPERIMENT_ID']}/comparison.csv\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eksperyment: Zwiększenie liczby epok dla `roberta-base`\n",
    "\n",
    "Tworzymy nową konfigurację `roberta-base-3` w locie, zmieniając liczbę epok treningowych, aby sprawdzić, czy dłuższy trening poprawi wynik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytanie bazowej konfiguracji\n",
    "with open('model_configs/roberta-base.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Modyfikacja hiperparametru\n",
    "config[\"training_arguments\"][\"num_train_epochs\"] = 3\n",
    "\n",
    "# Zapis nowej konfiguracji\n",
    "with open('model_configs/roberta-base-3.json', 'w') as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "\n",
    "print(\"Utworzono nową konfigurację: model_configs/roberta-base-3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uruchomienie eksperymentu dla nowej konfiguracji\n",
    "!uv run cli run-backtesting --experiment-id $EXPERIMENT_ID --model-config-name roberta-base-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ostateczne porównanie\n",
    "!uv run cli compare-models --experiment-id $EXPERIMENT_ID --output-file artifacts/experiments/$EXPERIMENT_ID/comparison.csv --run-type backtesting\n",
    "comparison_df = pd.read_csv(f\"artifacts/experiments/{os.environ['EXPERIMENT_ID']}/comparison.csv\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sekcja 4: Wybór Najlepszego Modelu\n",
    "\n",
    "Na podstawie powyższych wyników, jako najlepszy model do dalszych kroków wybieramy `roberta-base-3` (założenie na potrzeby tego tutoriala)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['BEST_MODEL'] = \"roberta-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sekcja 5: Oszacowanie Wydajności na Zbiorze Testowym\n",
    "\n",
    "Wybrany model sprawdzamy na odłożonym wcześniej zbiorze testowym. To da nam ostateczną, bezstronną ocenę jego jakości."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run cli estimate-performance --experiment-id $EXPERIMENT_ID --model-config-name $BEST_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sekcja 6: Trening Finalnego Modelu\n",
    "\n",
    "Mając pewność co do jakości naszego modelu, trenujemy go po raz ostatni na pełnym zbiorze danych (trening + walidacja), aby był gotowy do użycia produkcyjnego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run cli train-final-model --experiment-id $EXPERIMENT_ID --model-config-name $BEST_MODEL --model-output-dir artifacts/trained_models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sekcja 7: Predykcja na Nowych Danych\n",
    "\n",
    "Symulujemy scenariusz produkcyjny: pojawiły się nowe dane i chcemy poznać ich sentyment. Użyjemy do tego naszego generatora danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run cli data-generate --name new_unseen_data --examples-per-sentiment 5 --lang pl\n",
    "os.environ['NEW_DATA_FILE'] = \"artifacts/data/new_data/new_unseen_data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Używamy naszego finalnego modelu do wykonania predykcji na nowych danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_MODEL_PATH = f\"artifacts/trained_models/{os.environ['EXPERIMENT_ID']}_final/{os.environ['BEST_MODEL']}\"\n",
    "PREDICTION_OUTPUT_FILE = f\"artifacts/predictions/new_data_predictions.csv\"\n",
    "\n",
    "!uv run cli predict-new --model-path $FINAL_MODEL_PATH --input-file $NEW_DATA_FILE --output-file $PREDICTION_OUTPUT_FILE\n",
    "\n",
    "# Wyświetlenie predykcji\n",
    "predictions_df = pd.read_csv(PREDICTION_OUTPUT_FILE)\n",
    "print(\"Wygenerowane predykcje:\")\n",
    "display(predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sekcja 8: Symulacja Oceny Jakości\n",
    "\n",
    "W realnym scenariuszu dla nowo wygenerowanych danych nie mielibyśmy prawdziwych etykiet. Aby jednak zademonstrować działanie skryptu `evaluate.py`, zasymulujemy ocenę, traktując sentyment, o który prosiliśmy generator, jako \"prawdziwą etykietę\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate.py --predictions-file $PREDICTION_OUTPUT_FILE --ground-truth-file $NEW_DATA_FILE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
