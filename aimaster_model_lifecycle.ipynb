{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pełny Cykl Życia Modelu Sentymentu\n",
    "\n",
    "Ten notebook krok po kroku demonstruje cały cykl życia modelu do klasyfikacji sentymentu, od przygotowania danych, przez eksperymenty i wybór najlepszego modelu, aż po jego użycie do predykcji na nowych danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sekcja 1: Przygotowanie i Konfiguracja\n",
    "\n",
    "Zakładamy, że dane treningowe zostały już przygotowane i znajdują się w pliku `artifacts/data/all_train.csv`. W tej komórce definiujemy zmienne, których będziemy używać w całym procesie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "os.environ['EXPERIMENT_ID'] = \"exp_notebook_tutorial\"\n",
    "os.environ['INPUT_DATA'] = \"artifacts/data/all_train.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sekcja 2: Generowanie Podziałów Danych\n",
    "\n",
    "Tworzymy podziały danych dla naszego eksperymentu. Użyjemy strategii `train-val` z wydzielonym zbiorem testowym (20% danych), który posłuży do ostatecznej oceny najlepszego modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run cli generate-splits --experiment-id $EXPERIMENT_ID --input-file $INPUT_DATA --test-size 0.2 --backtesting-strategy train-val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sekcja 3: Uruchomienie Eksperymentów i Porównanie Modeli\n",
    "\n",
    "Teraz uruchomimy proces treningu i walidacji dla każdego z wybranych modeli. Po każdym przebiegu wyświetlimy zaktualizowaną tabelę z porównaniem wyników."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: svm-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run cli run-backtesting --experiment-id $EXPERIMENT_ID --model-config-name svm-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run cli compare-models --experiment-id $EXPERIMENT_ID --output-file artifacts/experiments/$EXPERIMENT_ID/comparison.csv --run-type backtesting\n",
    "comparison_df = pd.read_csv(f\"artifacts/experiments/{os.environ['EXPERIMENT_ID']}/comparison.csv\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: svm-opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run cli run-backtesting --experiment-id $EXPERIMENT_ID --model-config-name svm-opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run cli compare-models --experiment-id $EXPERIMENT_ID --output-file artifacts/experiments/$EXPERIMENT_ID/comparison.csv --run-type backtesting\n",
    "comparison_df = pd.read_csv(f\"artifacts/experiments/{os.environ['EXPERIMENT_ID']}/comparison.csv\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: Gemma without tuning (prompt-gemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!uv run cli run-backtesting --experiment-id $EXPERIMENT_ID --model-config-name prompt-gemma-3-1b-it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: bert-micro-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run cli run-backtesting --experiment-id $EXPERIMENT_ID --model-config-name bert-micro-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run cli compare-models --experiment-id $EXPERIMENT_ID --output-file artifacts/experiments/$EXPERIMENT_ID/comparison.csv --run-type backtesting\n",
    "comparison_df = pd.read_csv(f\"artifacts/experiments/{os.environ['EXPERIMENT_ID']}/comparison.csv\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run cli compare-models --experiment-id $EXPERIMENT_ID --output-file artifacts/experiments/$EXPERIMENT_ID/comparison.csv --run-type backtesting\n",
    "comparison_df = pd.read_csv(f\"artifacts/experiments/{os.environ['EXPERIMENT_ID']}/comparison.csv\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: bert-micro-long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run cli run-backtesting --experiment-id $EXPERIMENT_ID --model-config-name bert-micro-long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run cli compare-models --experiment-id $EXPERIMENT_ID --output-file artifacts/experiments/$EXPERIMENT_ID/comparison.csv --run-type backtesting\n",
    "comparison_df = pd.read_csv(f\"artifacts/experiments/{os.environ['EXPERIMENT_ID']}/comparison.csv\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run cli run-backtesting --experiment-id $EXPERIMENT_ID --model-config-name roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run cli compare-models --experiment-id $EXPERIMENT_ID --output-file artifacts/experiments/$EXPERIMENT_ID/comparison.csv --run-type backtesting\n",
    "comparison_df = pd.read_csv(f\"artifacts/experiments/{os.environ['EXPERIMENT_ID']}/comparison.csv\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eksperyment: Zwiększenie liczby epok dla `roberta-base`\n",
    "\n",
    "Tworzymy nową konfigurację `roberta-base-3` w locie, zmieniając liczbę epok treningowych, aby sprawdzić, czy dłuższy trening poprawi wynik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytanie bazowej konfiguracji\n",
    "with open('model_configs/roberta-base.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Modyfikacja hiperparametru\n",
    "config[\"training_arguments\"][\"num_train_epochs\"] = 3\n",
    "\n",
    "# Zapis nowej konfiguracji\n",
    "with open('model_configs/roberta-base-3.json', 'w') as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "\n",
    "print(\"Utworzono nową konfigurację: model_configs/roberta-base-3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uruchomienie eksperymentu dla nowej konfiguracji\n",
    "!uv run cli run-backtesting --experiment-id $EXPERIMENT_ID --model-config-name roberta-base-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ostateczne porównanie\n",
    "!uv run cli compare-models --experiment-id $EXPERIMENT_ID --output-file artifacts/experiments/$EXPERIMENT_ID/comparison.csv --run-type backtesting\n",
    "comparison_df = pd.read_csv(f\"artifacts/experiments/{os.environ['EXPERIMENT_ID']}/comparison.csv\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sekcja 4: Wybór Najlepszego Modelu\n",
    "\n",
    "Na podstawie powyższych wyników, jako najlepszy model do dalszych kroków wybieramy `roberta-base-3` (założenie na potrzeby tego tutoriala)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['BEST_MODEL'] = \"roberta-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sekcja 5: Oszacowanie Wydajności na Zbiorze Testowym\n",
    "\n",
    "Wybrany model sprawdzamy na odłożonym wcześniej zbiorze testowym. To da nam ostateczną, bezstronną ocenę jego jakości."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run cli estimate-performance --experiment-id $EXPERIMENT_ID --model-config-name $BEST_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sekcja 6: Trening Finalnego Modelu\n",
    "\n",
    "Mając pewność co do jakości naszego modelu, trenujemy go po raz ostatni na pełnym zbiorze danych (trening + walidacja), aby był gotowy do użycia produkcyjnego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run cli train-final-model --experiment-id $EXPERIMENT_ID --model-config-name $BEST_MODEL --model-output-dir artifacts/trained_models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sekcja 7: Predykcja na Nowych Danych\n",
    "\n",
    "Symulujemy scenariusz produkcyjny: pojawiły się nowe dane i chcemy poznać ich sentyment. Użyjemy do tego naszego generatora danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run cli data-generate --name new_unseen_data --examples-per-sentiment 5 --lang pl\n",
    "os.environ['NEW_DATA_FILE'] = \"artifacts/data/new_data/new_unseen_data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Używamy naszego finalnego modelu do wykonania predykcji na nowych danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_MODEL_PATH = f\"artifacts/trained_models/{os.environ['EXPERIMENT_ID']}_final/{os.environ['BEST_MODEL']}\"\n",
    "PREDICTION_OUTPUT_FILE = f\"artifacts/predictions/new_data_predictions.csv\"\n",
    "\n",
    "!uv run cli predict-new --model-path $FINAL_MODEL_PATH --input-file $NEW_DATA_FILE --output-file $PREDICTION_OUTPUT_FILE\n",
    "\n",
    "# Wyświetlenie predykcji\n",
    "predictions_df = pd.read_csv(PREDICTION_OUTPUT_FILE)\n",
    "print(\"Wygenerowane predykcje:\")\n",
    "display(predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sekcja 8: Symulacja Oceny Jakości\n",
    "\n",
    "W realnym scenariuszu dla nowo wygenerowanych danych nie mielibyśmy prawdziwych etykiet. Aby jednak zademonstrować działanie skryptu `evaluate.py`, zasymulujemy ocenę, traktując sentyment, o który prosiliśmy generator, jako \"prawdziwą etykietę\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate.py --predictions-file $PREDICTION_OUTPUT_FILE --ground-truth-file $NEW_DATA_FILE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
